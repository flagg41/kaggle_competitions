{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:03:54.149147Z",
     "start_time": "2021-01-10T19:03:54.113147Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "# import torch\n",
    "# from torchtext import data\n",
    "#import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "# from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os \n",
    "import nltk\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from textblob import TextBlob\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "import lightgbm as lgb\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:10:38.649956Z",
     "start_time": "2021-01-10T18:10:38.596922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:10:48.790832Z",
     "start_time": "2021-01-10T18:10:48.781804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick data analysis and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:12:38.098085Z",
     "start_time": "2021-01-10T18:12:38.086065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "sinking                  41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:12:50.310455Z",
     "start_time": "2021-01-10T18:12:50.295436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA              104\n",
       "New York          71\n",
       "United States     50\n",
       "London            45\n",
       "Canada            29\n",
       "                ... \n",
       "Tucson, Az         1\n",
       "Athens,Greece      1\n",
       "Wailuku, Maui      1\n",
       "my deli            1\n",
       "I O W A            1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:14:54.607623Z",
     "start_time": "2021-01-10T18:14:54.584958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 7613\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "print(train.shape[0], len(train.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:27:38.779724Z",
     "start_time": "2021-01-10T18:27:38.776717Z"
    }
   },
   "outputs": [],
   "source": [
    "# missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:30:17.892898Z",
     "start_time": "2021-01-10T18:30:17.882928Z"
    }
   },
   "source": [
    "# preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:45:05.343772Z",
     "start_time": "2021-01-10T18:45:05.298788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove punctuations:\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ')\n",
    "    return x\n",
    "\n",
    "# We won't clean numbers in conventional methods case since we might get extra info from bigrams like 5 mins or 30 mins\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "# Remove Misspell:\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "# remove stopwords:\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=True):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# remove contractions:\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Using lemmatizer to keep dictionary form of words. Might be helpful if later we want to use word embeddings.\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemma_text(text):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def clean_sentence(x):\n",
    "    x = x.lower()\n",
    "    x = clean_text(x)\n",
    "    x = replace_typical_misspell(x)\n",
    "    x = remove_stopwords(x)\n",
    "    x = replace_contractions(x)\n",
    "    x = lemma_text(x)\n",
    "    x = x.replace(\"'\",\"\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:54:46.117437Z",
     "start_time": "2021-01-10T18:54:43.462402Z"
    }
   },
   "outputs": [],
   "source": [
    "train['cleaned_text'] = train['text'].apply(lambda x : clean_sentence(x))\n",
    "test['cleaned_text'] = test['text'].apply(lambda x : clean_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:57:00.120037Z",
     "start_time": "2021-01-10T18:57:00.103037Z"
    }
   },
   "outputs": [],
   "source": [
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    # print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return tmp[2]\n",
    "\n",
    "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "\n",
    "        clf = copy.deepcopy(model_obj)\n",
    "        clf.fit(x_train_fold, y_train_fold)\n",
    "        valid_preds_fold = clf.predict_proba(x_val_fold)[:,1]\n",
    "\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds\n",
    "\n",
    "def lgb_model_train_cv(x_train,y_train,nfold,lgb):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "        d_train = lgb.Dataset(x_train_fold, label=y_train_fold)\n",
    "        d_val = lgb.Dataset(x_val_fold, label=y_val_fold)\n",
    "        params = {}\n",
    "        params['learning_rate'] = 0.01\n",
    "        params['boosting_type'] = 'gbdt'\n",
    "        params['objective'] = 'binary'\n",
    "        params['metric'] = 'binary_logloss'\n",
    "        params['sub_feature'] = 0.5\n",
    "        params['num_leaves'] = 10\n",
    "        params['min_data'] = 50\n",
    "        params['max_depth'] = 10\n",
    "        \n",
    "        clf = lgb.train(params, d_train, num_boost_round = 100,valid_sets=(d_val), early_stopping_rounds=10,verbose_eval=10)\n",
    "        valid_preds_fold = clf.predict(x_val_fold)\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:57:02.604494Z",
     "start_time": "2021-01-10T18:57:01.788977Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer(dtype=np.float32,\n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3),min_df=3)\n",
    "\n",
    "# Fitting count vectorizer to both training and test sets (semi-supervised learning)\n",
    "cnt_vectorizer.fit(list(train.cleaned_text.values) + list(test.cleaned_text.values))\n",
    "xtrain =  cnt_vectorizer.transform(train.cleaned_text.values) \n",
    "#xtest_cntv = cnt_vectorizer.transform(test_df.cleaned_text.values)\n",
    "y_train = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:09:10.769860Z",
     "start_time": "2021-01-10T19:09:10.761894Z"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:57:10.386170Z",
     "start_time": "2021-01-10T18:57:09.059169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmazur2\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\jmazur2\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108dc1ce23e14e06b24aed8ed95351ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.759 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n",
    "\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:02:19.506224Z",
     "start_time": "2021-01-10T19:02:19.323220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c6c71228444d899102645814f73582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.747 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:04:08.135987Z",
     "start_time": "2021-01-10T19:04:07.378982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2616, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 399\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429557 -> initscore=-0.283660\n",
      "[LightGBM] [Info] Start training from score -0.283660\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.674741\n",
      "[20]\tvalid_0's binary_logloss: 0.667414\n",
      "[30]\tvalid_0's binary_logloss: 0.660902\n",
      "[40]\tvalid_0's binary_logloss: 0.655023\n",
      "[50]\tvalid_0's binary_logloss: 0.65015\n",
      "[60]\tvalid_0's binary_logloss: 0.645911\n",
      "[70]\tvalid_0's binary_logloss: 0.642224\n",
      "[80]\tvalid_0's binary_logloss: 0.639153\n",
      "[90]\tvalid_0's binary_logloss: 0.636088\n",
      "[100]\tvalid_0's binary_logloss: 0.633498\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.633498\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429721 -> initscore=-0.282990\n",
      "[LightGBM] [Info] Start training from score -0.282990\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.673158\n",
      "[20]\tvalid_0's binary_logloss: 0.664588\n",
      "[30]\tvalid_0's binary_logloss: 0.656656\n",
      "[40]\tvalid_0's binary_logloss: 0.650051\n",
      "[50]\tvalid_0's binary_logloss: 0.644186\n",
      "[60]\tvalid_0's binary_logloss: 0.639078\n",
      "[70]\tvalid_0's binary_logloss: 0.634162\n",
      "[80]\tvalid_0's binary_logloss: 0.629977\n",
      "[90]\tvalid_0's binary_logloss: 0.626179\n",
      "[100]\tvalid_0's binary_logloss: 0.622957\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.622957\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429721 -> initscore=-0.282990\n",
      "[LightGBM] [Info] Start training from score -0.282990\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.673793\n",
      "[20]\tvalid_0's binary_logloss: 0.66523\n",
      "[30]\tvalid_0's binary_logloss: 0.656919\n",
      "[40]\tvalid_0's binary_logloss: 0.649796\n",
      "[50]\tvalid_0's binary_logloss: 0.643684\n",
      "[60]\tvalid_0's binary_logloss: 0.638167\n",
      "[70]\tvalid_0's binary_logloss: 0.633171\n",
      "[80]\tvalid_0's binary_logloss: 0.628913\n",
      "[90]\tvalid_0's binary_logloss: 0.625134\n",
      "[100]\tvalid_0's binary_logloss: 0.621477\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.621477\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 382\n",
      "[LightGBM] [Info] Number of data points in the train set: 6091, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429650 -> initscore=-0.283278\n",
      "[LightGBM] [Info] Start training from score -0.283278\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.673576\n",
      "[20]\tvalid_0's binary_logloss: 0.665315\n",
      "[30]\tvalid_0's binary_logloss: 0.657694\n",
      "[40]\tvalid_0's binary_logloss: 0.651153\n",
      "[50]\tvalid_0's binary_logloss: 0.645451\n",
      "[60]\tvalid_0's binary_logloss: 0.640161\n",
      "[70]\tvalid_0's binary_logloss: 0.635386\n",
      "[80]\tvalid_0's binary_logloss: 0.630928\n",
      "[90]\tvalid_0's binary_logloss: 0.627208\n",
      "[100]\tvalid_0's binary_logloss: 0.624044\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.624044\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 379\n",
      "[LightGBM] [Info] Number of data points in the train set: 6091, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429650 -> initscore=-0.283278\n",
      "[LightGBM] [Info] Start training from score -0.283278\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.672631\n",
      "[20]\tvalid_0's binary_logloss: 0.663416\n",
      "[30]\tvalid_0's binary_logloss: 0.655426\n",
      "[40]\tvalid_0's binary_logloss: 0.64888\n",
      "[50]\tvalid_0's binary_logloss: 0.64289\n",
      "[60]\tvalid_0's binary_logloss: 0.637584\n",
      "[70]\tvalid_0's binary_logloss: 0.632139\n",
      "[80]\tvalid_0's binary_logloss: 0.627739\n",
      "[90]\tvalid_0's binary_logloss: 0.623829\n",
      "[100]\tvalid_0's binary_logloss: 0.620477\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.620477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e09aae851ce47ab82d36817e8d67e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.656 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:05:37.966726Z",
     "start_time": "2021-01-10T19:05:37.186564Z"
    }
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(train.cleaned_text.values) + list(test.cleaned_text.values))\n",
    "xtrain =  tfv.transform(train.cleaned_text.values) \n",
    "#xtest_tfv = tfv.transform(test_df.cleaned_text.values)\n",
    "y_train = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:05:50.058966Z",
     "start_time": "2021-01-10T19:05:49.416963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e52adb392084d229664b4af72358ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.763 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n",
    "\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:05:57.006073Z",
     "start_time": "2021-01-10T19:05:56.819096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56adedbf70a14f32a82ebd9044f64536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.755 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:10:05.772407Z",
     "start_time": "2021-01-10T19:10:04.993473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2616, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429557 -> initscore=-0.283660\n",
      "[LightGBM] [Info] Start training from score -0.283660\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.675445\n",
      "[20]\tvalid_0's binary_logloss: 0.668781\n",
      "[30]\tvalid_0's binary_logloss: 0.662548\n",
      "[40]\tvalid_0's binary_logloss: 0.656745\n",
      "[50]\tvalid_0's binary_logloss: 0.651476\n",
      "[60]\tvalid_0's binary_logloss: 0.646909\n",
      "[70]\tvalid_0's binary_logloss: 0.642805\n",
      "[80]\tvalid_0's binary_logloss: 0.638968\n",
      "[90]\tvalid_0's binary_logloss: 0.63526\n",
      "[100]\tvalid_0's binary_logloss: 0.632428\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.632428\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3398\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429721 -> initscore=-0.282990\n",
      "[LightGBM] [Info] Start training from score -0.282990\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.674587\n",
      "[20]\tvalid_0's binary_logloss: 0.667166\n",
      "[30]\tvalid_0's binary_logloss: 0.660147\n",
      "[40]\tvalid_0's binary_logloss: 0.653629\n",
      "[50]\tvalid_0's binary_logloss: 0.647832\n",
      "[60]\tvalid_0's binary_logloss: 0.642769\n",
      "[70]\tvalid_0's binary_logloss: 0.63784\n",
      "[80]\tvalid_0's binary_logloss: 0.633394\n",
      "[90]\tvalid_0's binary_logloss: 0.628994\n",
      "[100]\tvalid_0's binary_logloss: 0.624612\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.624612\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3255\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429721 -> initscore=-0.282990\n",
      "[LightGBM] [Info] Start training from score -0.282990\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.674178\n",
      "[20]\tvalid_0's binary_logloss: 0.665798\n",
      "[30]\tvalid_0's binary_logloss: 0.658653\n",
      "[40]\tvalid_0's binary_logloss: 0.651889\n",
      "[50]\tvalid_0's binary_logloss: 0.646029\n",
      "[60]\tvalid_0's binary_logloss: 0.640922\n",
      "[70]\tvalid_0's binary_logloss: 0.636574\n",
      "[80]\tvalid_0's binary_logloss: 0.632112\n",
      "[90]\tvalid_0's binary_logloss: 0.628123\n",
      "[100]\tvalid_0's binary_logloss: 0.624264\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.624264\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3252\n",
      "[LightGBM] [Info] Number of data points in the train set: 6091, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429650 -> initscore=-0.283278\n",
      "[LightGBM] [Info] Start training from score -0.283278\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.674473\n",
      "[20]\tvalid_0's binary_logloss: 0.666468\n",
      "[30]\tvalid_0's binary_logloss: 0.659421\n",
      "[40]\tvalid_0's binary_logloss: 0.652936\n",
      "[50]\tvalid_0's binary_logloss: 0.647033\n",
      "[60]\tvalid_0's binary_logloss: 0.641876\n",
      "[70]\tvalid_0's binary_logloss: 0.637482\n",
      "[80]\tvalid_0's binary_logloss: 0.633149\n",
      "[90]\tvalid_0's binary_logloss: 0.629213\n",
      "[100]\tvalid_0's binary_logloss: 0.625454\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.625454\n",
      "[LightGBM] [Info] Number of positive: 2617, number of negative: 3474\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3204\n",
      "[LightGBM] [Info] Number of data points in the train set: 6091, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429650 -> initscore=-0.283278\n",
      "[LightGBM] [Info] Start training from score -0.283278\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.674334\n",
      "[20]\tvalid_0's binary_logloss: 0.665712\n",
      "[30]\tvalid_0's binary_logloss: 0.65786\n",
      "[40]\tvalid_0's binary_logloss: 0.650621\n",
      "[50]\tvalid_0's binary_logloss: 0.643668\n",
      "[60]\tvalid_0's binary_logloss: 0.637328\n",
      "[70]\tvalid_0's binary_logloss: 0.631563\n",
      "[80]\tvalid_0's binary_logloss: 0.626339\n",
      "[90]\tvalid_0's binary_logloss: 0.621458\n",
      "[100]\tvalid_0's binary_logloss: 0.616727\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.616727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8522780a607c4a02b0600c9f9bedfe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.653 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = lgb_model_train_cv(xtrain,y_train,5,lgb)\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:26:45.994625Z",
     "start_time": "2021-01-10T18:26:45.991590Z"
    }
   },
   "source": [
    "# not so simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:12:27.427062Z",
     "start_time": "2021-01-10T19:12:27.403056Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-31b595e19d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_glove_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found %s word vectors.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-31b595e19d0b>\u001b[0m in \u001b[0;36mload_glove_index\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "def load_glove_index():\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_index()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check\n",
    "https://www.analyticsvidhya.com/blog/2020/03/6-pretrained-models-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
